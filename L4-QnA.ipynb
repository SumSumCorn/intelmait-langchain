{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52824b89-532a-4e54-87e9-1410813cd39e",
   "metadata": {},
   "source": [
    "# LangChain: Q&A over Documents\n",
    "\n",
    "An example might be a tool that would allow you to query a product catalog for items of interest.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c1f7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --upgrade langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7ed03ed-1322-49e3-b2a2-33e94fb592ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "_ = load_dotenv(find_dotenv())  # read local .env file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead635a0-42e2-46cc-a9f7-98419eceae6d",
   "metadata": {},
   "source": [
    "Note: LLM's do not always produce the same results. When executing the code in your notebook, you may get slightly different answers that those in the video.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc533037-0b8c-4995-96a3-45b35fa13c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# account for deprecation of LLM model\n",
    "import datetime\n",
    "\n",
    "# Get the current date\n",
    "current_date = datetime.datetime.now().date()\n",
    "\n",
    "# Define the date after which the model should be set to \"gpt-3.5-turbo\"\n",
    "target_date = datetime.date(2024, 6, 12)\n",
    "\n",
    "# Set the model variable based on the current date\n",
    "if current_date > target_date:\n",
    "    llm_model = \"gpt-3.5-turbo\"\n",
    "else:\n",
    "    llm_model = \"gpt-3.5-turbo-0301\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "974acf8e-8f88-42de-88f8-40a82cb58e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7249846e",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"OutdoorClothingCatalog_100.csv\"\n",
    "loader = CSVLoader(\"./OutdoorClothingCatalog_100.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5bfaba30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.indexes import VectorstoreIndexCreator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b5ab657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install docarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e200726",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Could not import tiktoken python package. This is needed in order to for OpenAIEmbeddings. Please install it with `pip install tiktoken`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\MG\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain\\embeddings\\openai.py:322\u001b[0m, in \u001b[0;36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[1;34m(self, texts, engine, chunk_size)\u001b[0m\n\u001b[0;32m    321\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 322\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mtiktoken\u001b[39;00m\n\u001b[0;32m    323\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tiktoken'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\MG\\Desktop\\langchain\\L4-QnA.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/MG/Desktop/langchain/L4-QnA.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m index \u001b[39m=\u001b[39m VectorstoreIndexCreator(vectorstore_cls\u001b[39m=\u001b[39;49mDocArrayInMemorySearch)\u001b[39m.\u001b[39;49mfrom_loaders(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/MG/Desktop/langchain/L4-QnA.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     [loader]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/MG/Desktop/langchain/L4-QnA.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\MG\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain\\indexes\\vectorstore.py:82\u001b[0m, in \u001b[0;36mVectorstoreIndexCreator.from_loaders\u001b[1;34m(self, loaders)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[39mfor\u001b[39;00m loader \u001b[39min\u001b[39;00m loaders:\n\u001b[0;32m     81\u001b[0m     docs\u001b[39m.\u001b[39mextend(loader\u001b[39m.\u001b[39mload())\n\u001b[1;32m---> 82\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfrom_documents(docs)\n",
      "File \u001b[1;32mc:\\Users\\MG\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain\\indexes\\vectorstore.py:87\u001b[0m, in \u001b[0;36mVectorstoreIndexCreator.from_documents\u001b[1;34m(self, documents)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Create a vectorstore index from documents.\"\"\"\u001b[39;00m\n\u001b[0;32m     86\u001b[0m sub_docs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext_splitter\u001b[39m.\u001b[39msplit_documents(documents)\n\u001b[1;32m---> 87\u001b[0m vectorstore \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvectorstore_cls\u001b[39m.\u001b[39;49mfrom_documents(\n\u001b[0;32m     88\u001b[0m     sub_docs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membedding, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvectorstore_kwargs\n\u001b[0;32m     89\u001b[0m )\n\u001b[0;32m     90\u001b[0m \u001b[39mreturn\u001b[39;00m VectorStoreIndexWrapper(vectorstore\u001b[39m=\u001b[39mvectorstore)\n",
      "File \u001b[1;32mc:\\Users\\MG\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain\\vectorstores\\base.py:417\u001b[0m, in \u001b[0;36mVectorStore.from_documents\u001b[1;34m(cls, documents, embedding, **kwargs)\u001b[0m\n\u001b[0;32m    415\u001b[0m texts \u001b[39m=\u001b[39m [d\u001b[39m.\u001b[39mpage_content \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m documents]\n\u001b[0;32m    416\u001b[0m metadatas \u001b[39m=\u001b[39m [d\u001b[39m.\u001b[39mmetadata \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m documents]\n\u001b[1;32m--> 417\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mfrom_texts(texts, embedding, metadatas\u001b[39m=\u001b[39;49mmetadatas, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\MG\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain\\vectorstores\\docarray\\in_memory.py:68\u001b[0m, in \u001b[0;36mDocArrayInMemorySearch.from_texts\u001b[1;34m(cls, texts, embedding, metadatas, **kwargs)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Create an DocArrayInMemorySearch store and insert data.\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \n\u001b[0;32m     55\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[39m    DocArrayInMemorySearch Vector Store\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     67\u001b[0m store \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mfrom_params(embedding, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m---> 68\u001b[0m store\u001b[39m.\u001b[39;49madd_texts(texts\u001b[39m=\u001b[39;49mtexts, metadatas\u001b[39m=\u001b[39;49mmetadatas)\n\u001b[0;32m     69\u001b[0m \u001b[39mreturn\u001b[39;00m store\n",
      "File \u001b[1;32mc:\\Users\\MG\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain\\vectorstores\\docarray\\base.py:82\u001b[0m, in \u001b[0;36mDocArrayIndex.add_texts\u001b[1;34m(self, texts, metadatas, **kwargs)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Embed texts and add to the vector store.\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \n\u001b[0;32m     74\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[39m    List of ids from adding the texts into the vectorstore.\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     81\u001b[0m ids: List[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m []\n\u001b[1;32m---> 82\u001b[0m embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membedding\u001b[39m.\u001b[39;49membed_documents(\u001b[39mlist\u001b[39;49m(texts))\n\u001b[0;32m     83\u001b[0m \u001b[39mfor\u001b[39;00m i, (t, e) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mzip\u001b[39m(texts, embeddings)):\n\u001b[0;32m     84\u001b[0m     m \u001b[39m=\u001b[39m metadatas[i] \u001b[39mif\u001b[39;00m metadatas \u001b[39melse\u001b[39;00m {}\n",
      "File \u001b[1;32mc:\\Users\\MG\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain\\embeddings\\openai.py:483\u001b[0m, in \u001b[0;36mOpenAIEmbeddings.embed_documents\u001b[1;34m(self, texts, chunk_size)\u001b[0m\n\u001b[0;32m    471\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Call out to OpenAI's embedding endpoint for embedding search docs.\u001b[39;00m\n\u001b[0;32m    472\u001b[0m \n\u001b[0;32m    473\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    479\u001b[0m \u001b[39m    List of embeddings, one for each text.\u001b[39;00m\n\u001b[0;32m    480\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    481\u001b[0m \u001b[39m# NOTE: to keep things simple, we assume the list may contain texts longer\u001b[39;00m\n\u001b[0;32m    482\u001b[0m \u001b[39m#       than the maximum context and use length-safe embedding function.\u001b[39;00m\n\u001b[1;32m--> 483\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_len_safe_embeddings(texts, engine\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdeployment)\n",
      "File \u001b[1;32mc:\\Users\\MG\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain\\embeddings\\openai.py:324\u001b[0m, in \u001b[0;36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[1;34m(self, texts, engine, chunk_size)\u001b[0m\n\u001b[0;32m    322\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mtiktoken\u001b[39;00m\n\u001b[0;32m    323\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[1;32m--> 324\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[0;32m    325\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCould not import tiktoken python package. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    326\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThis is needed in order to for OpenAIEmbeddings. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease install it with `pip install tiktoken`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    328\u001b[0m     )\n\u001b[0;32m    330\u001b[0m tokens \u001b[39m=\u001b[39m []\n\u001b[0;32m    331\u001b[0m indices \u001b[39m=\u001b[39m []\n",
      "\u001b[1;31mImportError\u001b[0m: Could not import tiktoken python package. This is needed in order to for OpenAIEmbeddings. Please install it with `pip install tiktoken`."
     ]
    }
   ],
   "source": [
    "index = VectorstoreIndexCreator(vectorstore_cls=DocArrayInMemorySearch).from_loaders(\n",
    "    [loader]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34562d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Please list all your shirts with sun protection \\\n",
    "in a table in markdown and summarize each one.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd0cc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = index.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae21f1ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2534597e-4b0c-4563-a208-e2dd91064438",
   "metadata": {},
   "source": [
    "## Step By Step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631396c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import CSVLoader\n",
    "\n",
    "loader = CSVLoader(file_path=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2164b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a977f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e875693a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779bec75",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = embeddings.embed_query(\"Hi my name is Harrison\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699aaaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(embed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d00d346",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embed[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ad0bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DocArrayInMemorySearch.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0329bfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Please suggest a shirt with sunblocking\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7909c6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = db.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43321853",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eba90b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c3596e",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0625f5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.0, model=llm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a573f58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdocs = \"\".join([docs[i].page_content for i in range(len(docs))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14682d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.call_as_llm(\n",
    "    f\"{qdocs} Question: Please list all your \\\n",
    "shirts with sun protection in a table in markdown and summarize each one.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bba545b",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c94d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_stuff = RetrievalQA.from_chain_type(\n",
    "    llm=llm, chain_type=\"stuff\", retriever=retriever, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4769316",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Please list all your shirts with sun protection in a table \\\n",
    "in markdown and summarize each one.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc3c2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = qa_stuff.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba1a5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500ec062",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = index.query(query, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cffb19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorstoreIndexCreator(\n",
    "    vectorstore_cls=DocArrayInMemorySearch,\n",
    "    embedding=embeddings,\n",
    ").from_loaders([loader])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b325340f-26b4-4c7e-81da-da4b895ae058",
   "metadata": {},
   "source": [
    "Reminder: Download your notebook to you local computer to save your work.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b58916",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d590b337",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cb587c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec249f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d64f166",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21322e7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
